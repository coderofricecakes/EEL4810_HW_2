# -*- coding: utf-8 -*-
"""EEL_4810_HW_2 _FINAL(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JCEucU6-DbR6qL25g_fv4S7ar1q7wbEz

#Imports Libraries
"""

#Cell 1

#Install necessary libraries
!pip install pytorch-lightning  # Framework from Pytroch
!pip install torchmetrics  # F1 and Loss and Accuracy
!pip install transformers torchmetrics

import torch  #Core Library
import torch.nn as nn  #Neural Network
from torch.utils.data import DataLoader, Dataset  #Handling datasets and batches
import os
from torchmetrics import Accuracy, F1Score  #The Metrics
import pandas as pd  #CSV reading
import numpy as np  #Numerical operations
import matplotlib.pyplot as plt  #Plotting losses


#Features is equivalent to input
#Labels is equivalent to expected output


#Quick note about this code, it was formatted to work with google drive
#You will have to go into cell 5 and change the file location on where your .csv files are and where you want to save the model
#Other then that you can go to cell 5 and see all the spots where it says "change me" and thats how you can get the different configurations

"""#Dataset"""

#Cell 2

#Define custom dataset for CSV data
class EEL4810_HW2_Dataset(Dataset):

    def __init__(self, data_dir, normalize=False):
        self.data_dir = data_dir  #Root directory for CSV data
        self.features = []  #Store feature data
        self.labels = []  #Store labels
        self.normalize = normalize  #Flag to enable/disable normalization (Part 2.1)
        subfolders = ['sub01', 'sub02', 'sub03', 'sub05']  #List of subfolders containing CSV files instead of the categories

        #loop through subfolders to import the CSV files and assign the labels
        for i in range(len(subfolders)):
            subfolder_name = subfolders[i]
            folder = os.path.join(data_dir, subfolder_name)  #Path to subfolder
            files = os.listdir(folder)


            for file in files:

                if file.endswith('.csv'):  #Only process .csv files
                    file_path = os.path.join(folder, file)
                    df = pd.read_csv(file_path, header= None)  #Read CSV file
                    #Clean feature columns (indices 1, 2, 3) by replacing '--1' with -1

                    for col in [1, 2, 3]:
                        df[col] = df[col].replace('--1', -1) #One input value had a double negative

                    #Convert label column to string to handle '#'
                    df[4] = df[4].astype(str)

                    #Replace '#' with 3 (new class)
                    df[4] = df[4].replace('#', '3')

                    #Extract features and labels
                    features = df.iloc[:, [1, 2, 3]].values.astype(np.float32)
                    labels = df[4].values.astype(np.int64)

                    #Apply standard scaling if normalize is True (Part 2.1: With and without normalization)
                    if self.normalize:
                        mean = np.mean(features, axis = 0)  #Compute mean for each feature
                        std = np.std(features, axis = 0)  #Compute standard deviation for each feature
                        features = (features - mean) / (std + 1e-8)  #Standard scaling: (x - mean) / std

                    #Append features and labels
                    self.features.append(features)
                    self.labels.append(labels)

        #Concatenate all features and labels
        self.features = np.concatenate(self.features, axis= 0)
        self.labels = np.concatenate(self.labels, axis = 0)

        #Convert to tensors
        self.features = torch.FloatTensor(self.features)
        self.labels = torch.LongTensor(self.labels)

    def __len__(self):
        return len(self.features)  #Return the total number of samples

    def __getitem__(self, index):
        feature = self.features[index]  #Get features at the given index
        label = self.labels[index]  #Get corresponding label
        return feature, label

"""#Model"""

#Cell 3

#Define custom model using a 5-layer neural network
class EEL4810_H2_Model(nn.Module):

    def __init__(self):

        super(EEL4810_H2_Model, self).__init__()

        #Define the 5-layer network
        self.layer1 = nn.Linear(3, 256)  #Input layer: 3 features to 256 units
        self.layer2 = nn.Linear(256, 256)  #Hidden layer
        self.layer3 = nn.Linear(256, 256)  #Hidden layer
        self.layer4 = nn.Linear(256, 256)  #Hidden layer
        self.final_layer = nn.Linear(256, 4)  #Output layer: 4 classes (0, 1, 2, 3)
        self.relu = nn.ReLU()  #Activation function for non-linearity


        #Weight initialization (Part 2.5: Random vs Xavier)
        self.init_method = 'xavier'  #Default to xavier (toggle in Cell 5))

        if self.init_method == 'xavier':  #Here is Xavier initiallization
            for layer in [self.layer1, self.layer2, self.layer3, self.layer4, self.final_layer]:
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
        else:  #Here is random initialization
            for layer in [self.layer1, self.layer2, self.layer3, self.layer4, self.final_layer]:
                nn.init.normal_(layer.weight, mean = 0, std = 0.01)
                nn.init.zeros_(layer.bias)

    def forward(self, input_features):

        #Pass features through the layers with ReLU activation
        hidden = self.layer1(input_features)
        activated = self.relu(hidden)  #Activate ReLU
        hidden = self.layer2(activated)
        activated = self.relu(hidden)
        hidden = self.layer3(activated)
        activated = self.relu(hidden)
        hidden = self.layer4(activated)
        activated = self.relu(hidden)
        logits = self.final_layer(activated)


        return logits

"""#Training Function"""

#Cell 4

from torchmetrics import Accuracy, F1Score


def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs):

    # use GPU if available
    if torch.cuda.is_available():
        model = model.cuda()

    class_count = 4  # the number of classes in the dataset

    deviceUsed = "cuda" if torch.cuda.is_available() else "cpu"

    #Metrics for training and validation
    train_accuracy_metric = Accuracy(task="multiclass" , num_classes = class_count).to(deviceUsed)
    val_accuracy_metric = Accuracy(task="multiclass", num_classes = class_count).to(deviceUsed)
    f1_metric = F1Score(task="multiclass", num_classes =class_count, average = 'macro').to(deviceUsed)


    #Lists to store training and validation losses for each epoch
    train_losses = []
    val_losses = []

    #Training loop over number of epochs
    for numEpoch in range(epochs):

        model.train()  #Set model to training mode
        train_loss = 0
        train_accuracy_metric.reset()
        f1_metric.reset()

        #Iterate over training batches
        for features, labels in train_loader:
            #Move to cuda if available
            if torch.cuda.is_available():
                features = features.cuda()
                labels = labels.cuda()
            optimizer.zero_grad()
            outputs = model(features)  #Forward pass
            loss= criterion(outputs, labels)  #Compute Loss
            loss.backward()  #Backpropagation
            optimizer.step()  #Update the weights
            train_loss += loss.item()  #get total loss
            preds = torch.argmax(outputs, dim = 1)  #Get predicted class
            train_accuracy_metric.update(preds, labels)  #Update accuracy
            f1_metric.update(preds, labels)  #Update F1 Score

        #Calculate average metrics for training
        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)  #Store the loss for this epoch
        train_accuracy = train_accuracy_metric.compute().item()

        #Calculate the average F1 score for the training data
        train_f1 = f1_metric.compute().item()

        model.eval()  #Put model in Evaluation mode
        val_loss = 0  #Track validation loss
        val_accuracy_metric.reset()  #Reset validation accuracy
        f1_metric.reset()  #Reset F1 metric

        #Validation loop
        with torch.no_grad():
            for features, labels in test_loader:

                #Move to cuda if available
                if torch.cuda.is_available():
                    features = features.cuda()
                    labels = labels.cuda()


                outputs = model(features)  #Forward pass
                loss = criterion(outputs, labels)  #Compute loss
                val_loss += loss.item()  #Accumulate loss
                preds = torch.argmax(outputs, dim=1)  #Get predicted class
                val_accuracy_metric.update(preds, labels)  #Update accuracy
                f1_metric.update(preds, labels)  #New F1 score

        #Compute average metrics for validation
        avg_val_loss = val_loss / len(test_loader)
        val_accuracy = val_accuracy_metric.compute().item()
        val_f1 = f1_metric.compute().item()
        val_losses.append(avg_val_loss)  #Store the loss for this epoch

        #Results printed to user for the epochs
        print(f"Epoch {numEpoch + 1}:")
        print(f"  Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1: {train_f1:.4f}")
        print(f"  Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}")

    #Plot training and validation losses
    plt.plot(range(1, epochs + 1), train_losses, label = 'Train Loss')  #Plot training loss trend
    plt.plot(range(1, epochs + 1), val_losses, label ='Validation Loss')  #Plot validation loss trend
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    return model

"""#Execution"""

#Cell 5

#CHANGE TO YOUR OWN FILE LOCATION

from google.colab import drive
drive.mount('/content/drive')  #Mount Google Drive to access data (CHANGE TO YOUR OWN FILE LOCATION)
data_dir = "/content/drive/MyDrive/eel4810-dataset"  #Directory of the CSV data (CHANGE TO YOUR OWN FILE LOCATION)




#CHANGE ME Toggle normalize=True/False for Part 2.1 experiment (with and without normalization) (False = no normalization, True = normalization)
dataset = EEL4810_HW2_Dataset(data_dir, normalize=True)





#Split data set into training and testing (90/10 split as per assignment)
train_size = int(0.9 * len(dataset))
test_size = len(dataset) - train_size
train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])








#CHANGE ME Create data loaders for batching (Part 2.2: Different mini-batches) (Comment one out)
mini_batch_size_1 = 16  #Here is mini-batch size 1 (matches original batch size)
train_loader = DataLoader(train_data, batch_size=mini_batch_size_1, shuffle=True)  #Shuffle for training
test_loader = DataLoader(test_data, batch_size=mini_batch_size_1, shuffle=False)  #No shuffle for testing



'''
mini_batch_size_2 = 32  #Here is mini-batch size 2 (comment out one)
train_loader = DataLoader(train_data, batch_size=mini_batch_size_2, shuffle=True)  #Shuffle for training
test_loader = DataLoader(test_data, batch_size=mini_batch_size_2, shuffle=False)  #No shuffle for testing
'''








model = EEL4810_H2_Model()  #Initiate the model











#CHANGE ME Set init_method (Part 2.5: Weight initialization)
model.init_method = 'xavier'  #Use either 'xavier' or 'random'
if model.init_method == 'xavier':
    for layer in [model.layer1, model.layer2, model.layer3, model.layer4, model.final_layer]:
        nn.init.xavier_uniform_(layer.weight)
        nn.init.zeros_(layer.bias)
else:
    for layer in [model.layer1, model.layer2, model.layer3, model.layer4, model.final_layer]:
        nn.init.normal_(layer.weight, mean=0, std=0.01)
        nn.init.zeros_(layer.bias)








criterion = nn.CrossEntropyLoss()  #Define the loss function









#CHANGE ME Define learning rates (Part 2.3: Two different learning rates) (Comment one of them out)
learning_rate_1 = 0.001  #Here is learning rate 1           (Change the learning rate value to either 0.01 or 0.001)










#CHANGE ME Define weight decay (Part 2.6: With and without L2 regularization)
l2_lambda = 0.1  #Here is L2 regularization (Change to 0 for no regularization, tested with 0.1)








#CHANGE ME Define optimizers (Part 2.4: Different optimizers) (Comment in the optimizer you want to you, use one at a time)
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate_1, momentum=0.9, weight_decay=l2_lambda)  #Momentum
#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_1, weight_decay=l2_lambda)  #Adam
#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate_1, weight_decay=l2_lambda)  #RMSProp
#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate_1, weight_decay=l2_lambda)  #SGD





model = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs=50)  #You can change the epoch here
torch.save(model.state_dict(), '/content/drive/MyDrive/eel4810_model.pth')
print("Saved the model to my Drive!")  #PLEASE SAVE

"""#Test Script"""

#Cell 6

from google.colab import drive
drive.mount('/content/drive')  #Mount drive AGAIN to gain access to the saved model
model = EEL4810_H2_Model()  #Create new model

#Ensure the init_method matches the training configuration
model.init_method = 'xavier'

if model.init_method == 'xavier':
    for layer in [model.layer1, model.layer2, model.layer3, model.layer4, model.final_layer]:
        nn.init.xavier_uniform_(layer.weight)
        nn.init.zeros_(layer.bias)
else:
    for layer in [model.layer1, model.layer2, model.layer3, model.layer4, model.final_layer]:
        nn.init.normal_(layer.weight, mean=0, std=0.01)
        nn.init.zeros_(layer.bias)

#Load saved weights into the model
model.load_state_dict(torch.load('/content/drive/MyDrive/eel4810_model.pth'))  #Location of where the model is

deviceUsed = "cuda" if torch.cuda.is_available() else "cpu"

#Move to cuda if available
if torch.cuda.is_available():
    model = model.cuda()

model.eval()

#Test on the dataset using the test loader
print("Testing on the test dataset!")  #Message printed to user

#Initialize accuracy metric
accuracy_metric = Accuracy(task = "multiclass", num_classes = 4).to(deviceUsed)
test_loss = 0

#Perform inference (inference is when the saved model is loaded, the test data is classified, and the result is printed)
with torch.no_grad():
    for features, labels in test_loader:
        if torch.cuda.is_available():
            features = features.cuda()
            labels = labels.cuda()

        outputs = model(features)  #Call the model
        loss = criterion(outputs, labels)
        test_loss += loss.item()
        preds = torch.argmax(outputs, dim =1)  #Get class index
        accuracy_metric.update(preds, labels)

#Compute average test loss and accuracy
avg_test_loss = test_loss / len(test_loader)
test_accuracy = accuracy_metric.compute().item()
print(f"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")  #Print result to the user

